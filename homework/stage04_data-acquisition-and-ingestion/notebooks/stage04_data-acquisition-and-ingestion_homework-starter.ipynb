{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c385a7-f219-48c3-b0ff-addcb97f932a",
   "metadata": {},
   "source": [
    "# Stage 04 — Data Acquisition & Ingestion\n",
    "\n",
    "**Goals**\n",
    "- API pull (with `.env` for key if needed) → validate → save to `data/raw/`\n",
    "- Scrape a small public table with BeautifulSoup → validate → save to `data/raw/`\n",
    "- Docs: sources/params/validation + assumptions & risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5845384d-bf28-4b9d-b595-afe86c056d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chen/bootcamp_tingchen_chen/homework/homework4/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/fe-course/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os; print(os.getcwd())\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630df4f1-4334-47a9-921a-2ada2609eb7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/chen/bootcamp_tingchen_chen/homework/homework4/notebooks\n",
      "SRC_PATH: /Users/chen/bootcamp_tingchen_chen/homework/homework4/src exists? True\n",
      "RAW_DIR: /Users/chen/bootcamp_tingchen_chen/homework/homework4/data/raw\n"
     ]
    }
   ],
   "source": [
    "# Cell 2：設定匯入與路徑（從 notebooks/ 相對到 ../src 與 ../data/raw）\n",
    "from pathlib import Path\n",
    "import sys, os, importlib\n",
    "\n",
    "NB_DIR = Path.cwd()                 # .../homework/homework4/notebooks\n",
    "HW4_DIR = NB_DIR.parent             # .../homework/homework4\n",
    "SRC_PATH = (HW4_DIR / \"src\").resolve()\n",
    "RAW_DIR = (HW4_DIR / \"data\" / \"raw\").resolve()\n",
    "\n",
    "# 讓專案的 src 走在最前面，避免撞到 site-packages 的 utils\n",
    "sys.path.insert(0, str(SRC_PATH))\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "print(\"cwd:\", NB_DIR)\n",
    "print(\"SRC_PATH:\", SRC_PATH, \"exists?\", SRC_PATH.exists())\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "\n",
    "# 匯入你自己的工具函式（檔名是 hw4_utils.py）\n",
    "from hw4_utils import load_api_key, ensure_dtypes, validate_df, save_raw_csv\n",
    "\n",
    "# 建立 raw 資料夾\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f28802-d461-41d3-9631-40e6d8aff647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['open', 'high', 'low', 'close', 'volume']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ok': True,\n",
       " 'missing_cols': [],\n",
       " 'na_counts': {'open': 0, 'high': 0, 'low': 0, 'close': 0, 'volume': 0},\n",
       " 'shape': (251, 5)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "TICKER   = \"AAPL\"    # 可改\n",
    "PERIOD   = \"1y\"\n",
    "INTERVAL = \"1d\"\n",
    "\n",
    "# 1) 抓資料（固定不自動調整，保留 Adj Close 欄；也關掉進度條）\n",
    "data = yf.download(TICKER, period=PERIOD, interval=INTERVAL, auto_adjust=False, progress=False)\n",
    "\n",
    "# 2) 若是 MultiIndex 欄位就攤平；否則直接用\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    data.columns = ['_'.join([str(x) for x in tup if x]).strip().lower() for tup in data.columns]\n",
    "else:\n",
    "    data.columns = [str(c).strip().lower().replace(' ', '_') for c in data.columns]\n",
    "\n",
    "# 3) reset index 並把日期欄標準化\n",
    "df_api = data.reset_index()\n",
    "df_api.rename(columns={\"date\": \"date\", \"datetime\": \"date\"}, inplace=True)\n",
    "# yfinance 可能把日期欄叫 \"date\" 或 \"index\"；補救：\n",
    "if \"date\" not in df_api.columns and \"index\" in df_api.columns:\n",
    "    df_api.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "\n",
    "# 4) 把常見價量欄位對齊成一套命名（若原本就小寫，這步不會動到）\n",
    "rename_map = {\n",
    "    \"open\": \"open\",\n",
    "    \"high\": \"high\",\n",
    "    \"low\": \"low\",\n",
    "    \"close\": \"close\",\n",
    "    \"adj_close\": \"adj_close\",\n",
    "    \"volume\": \"volume\",\n",
    "    \"open_aapl\": \"open\", \"high_aapl\": \"high\", \"low_aapl\": \"low\",\n",
    "    \"close_aapl\": \"close\", \"adj_close_aapl\": \"adj_close\", \"volume_aapl\": \"volume\",\n",
    "}\n",
    "df_api.rename(columns={k: v for k, v in rename_map.items() if k in df_api.columns}, inplace=True)\n",
    "\n",
    "# 5) 只保留我們關心的欄（存在才留）\n",
    "wanted = [\"date\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
    "present = [c for c in wanted if c in df_api.columns]\n",
    "df_api = df_api.loc[:, present]\n",
    "\n",
    "# 6) 型別處理（只對真的存在的欄位做）\n",
    "float_cols = [c for c in [\"open\", \"high\", \"low\", \"close\", \"adj_close\"] if c in df_api.columns]\n",
    "df_api = ensure_dtypes(df_api, date_cols=[\"date\"], float_cols=float_cols)\n",
    "\n",
    "# 7) 驗證（至少 5 列、必要欄位以實際存在為準）\n",
    "required = [c for c in [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"] if c in df_api.columns]\n",
    "report_api = validate_df(df_api, required_cols=required, min_rows=5)\n",
    "print(\"columns:\", df_api.columns.tolist())\n",
    "report_api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1750f2a1-9306-46e0-8afe-10a872337a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/chen/bootcamp_tingchen_chen/homework/homework4/data/raw/api_yfinance_aapl_20250819-1024.csv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_path = save_raw_csv(df_api, kind=\"api\", source=\"yfinance\", tag=TICKER, outdir=RAW_DIR)\n",
    "api_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bab59d-8e42-424f-9eea-a4c4a30a203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICSSector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>0000066740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>0000091142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>0000001800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol             Security   GICSSector         GICS Sub-Industry  \\\n",
       "0    MMM                   3M  Industrials  Industrial Conglomerates   \n",
       "1    AOS          A. O. Smith  Industrials         Building Products   \n",
       "2    ABT  Abbott Laboratories  Health Care     Health Care Equipment   \n",
       "\n",
       "     Headquarters Location Date added         CIK Founded  \n",
       "0    Saint Paul, Minnesota 1957-03-04  0000066740    1902  \n",
       "1     Milwaukee, Wisconsin 2017-07-26  0000091142    1916  \n",
       "2  North Chicago, Illinois 1957-03-04  0000001800    1888  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"  # 可換其他允許公開的表格頁\n",
    "SITE = \"wikipedia\"\n",
    "TAG  = \"sp500_table\"\n",
    "\n",
    "html = requests.get(URL, timeout=30).text\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "table = soup.find(\"table\", {\"class\": \"wikitable\"})  # 依頁面調整 selector\n",
    "assert table is not None, \"找不到表格，請檢查 selector\"\n",
    "\n",
    "# headers\n",
    "ths = table.find(\"tr\").find_all([\"th\", \"td\"])\n",
    "columns = [th.get_text(strip=True) for th in ths]\n",
    "\n",
    "# rows\n",
    "rows = []\n",
    "for tr in table.find_all(\"tr\")[1:]:\n",
    "    tds = tr.find_all([\"td\",\"th\"])\n",
    "    if not tds:\n",
    "        continue\n",
    "    rows.append([td.get_text(strip=True) for td in tds])\n",
    "\n",
    "df_scrape = pd.DataFrame(rows, columns=columns[:len(rows[0])] if rows else columns)\n",
    "\n",
    "# 簡單型別處理（日期欄、數值欄可依實際欄名調整）\n",
    "for c in df_scrape.columns:\n",
    "    if \"date\" in c.lower():\n",
    "        df_scrape[c] = pd.to_datetime(df_scrape[c], errors=\"coerce\")\n",
    "df_scrape.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8762fd64-1be0-496d-8981-dcb01dacd498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/chen/bootcamp_tingchen_chen/homework/homework4/data/raw/scrape_wikipedia_sp500_table_20250819-1024.csv')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_cols = df_scrape.columns[:3].tolist()   # 至少要求幾個欄位存在；可改成你需要的欄名\n",
    "report_scrape = validate_df(df_scrape, required_cols=required_cols, min_rows=5)\n",
    "report_scrape\n",
    "\n",
    "scrape_path = save_raw_csv(df_scrape, kind=\"scrape\", source=SITE, tag=TAG, outdir=RAW_DIR)\n",
    "scrape_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93badbfa-e570-4132-98ba-d812ccba081f",
   "metadata": {},
   "source": [
    "## Sources, Parameters, and Validation\n",
    "\n",
    "**API**\n",
    "- Source: yfinance  \n",
    "- Parameters: ticker=`AAPL`, period=`1y`, interval=`1d`, auto_adjust=False  \n",
    "- Validation: required columns = [`date`, `open`, `high`, `low`, `close`, `volume`]; checked with `report_api` (row count, NA values).  \n",
    "\n",
    "**Scrape**\n",
    "- URL: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies  \n",
    "- Selector: `<table class=\"wikitable\">` (first occurrence)  \n",
    "- Validation: required columns = first three columns of the table; checked with `report_scrape` (row count, NA values).  \n",
    "\n",
    "**Reproducible Filenames**\n",
    "- `data/raw/api_yfinance_AAPL_YYYYMMDD-HHMM.csv`  \n",
    "- `data/raw/scrape_wikipedia_sp500_table_YYYYMMDD-HHMM.csv`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81de6a-e577-479a-beeb-1f4da903aa75",
   "metadata": {},
   "source": [
    "## Assumptions & Risks\n",
    "- API schema and column names may change over time; cleaning and validation logic must be updated accordingly.  \n",
    "- Scraping depends on the HTML structure (table class and column order); if the webpage changes, selectors and parsing logic must be revised.  \n",
    "- `.env` file is for local use only; only `.env.example` is included in the repo. Sensitive API keys must not be committed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98682fc-93c5-4dbc-b9b2-fe0370ae35ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
